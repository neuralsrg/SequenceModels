{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPZ6wUUpNp0N1KZjE4XFoRZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuralsrg/SequenceModels/blob/main/machine_translation/NMT_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Translation with Attention Model"
      ],
      "metadata": {
        "id": "2DYRHpm_nP7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install faker"
      ],
      "metadata": {
        "id": "uZWnacYeYU8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d39878a-7e6b-40d2-d187-1522efe9b312"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faker\n",
            "  Downloading Faker-13.15.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0.2 in /usr/local/lib/python3.7/dist-packages (from faker) (4.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.7/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.4->faker) (1.15.0)\n",
            "Installing collected packages: faker\n",
            "Successfully installed faker-13.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SnYW0QChYGlb"
      },
      "outputs": [],
      "source": [
        "from faker import Faker # generates random dates\n",
        "import numpy as np \n",
        "from tqdm import trange\n",
        "import babel.dates\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate dates dataset"
      ],
      "metadata": {
        "id": "INILPObTnMYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(m : int):\n",
        "\n",
        "  generator = Faker()\n",
        "  FORMATS = ['short', 'medium', 'long', 'full', 'd MM YY', 'd MMM YYY',\n",
        "             'd MMMM YYY', 'd MMM, YYY', 'd MMMM, YYY', 'dd, MMM YYY', 'd MM YY',\n",
        "             'MMMM d YYY', 'MMMM d, YYY', 'dd.MM.YY']\n",
        "  PROBS = [.04, .04, .04, .44, .04, .08, .04, .04, .04, .04, .04, 0.04, .04, .04]\n",
        "\n",
        "  Tx = 30\n",
        "  dataset = []\n",
        "  human = set()\n",
        "  machine = set()\n",
        "\n",
        "  for i in trange(m):\n",
        "    date_object = generator.date_object()\n",
        "\n",
        "    human_readable = babel.dates.format_date(date_object,\n",
        "                                              format=np.random.choice(FORMATS, p=PROBS),\n",
        "                                              locale='en_US')\n",
        "    human_readable = human_readable.lower().replace(',','')\n",
        "    machine_readable = date_object.isoformat()\n",
        "\n",
        "    dataset.append((human_readable, machine_readable))\n",
        "    human.update(tuple(human_readable))\n",
        "    machine.update(tuple(machine_readable))\n",
        "\n",
        "  human = dict(zip(sorted(human) + ['<unk>', '<pad>'], \n",
        "                     list(range(len(human) + 2))))\n",
        "  inv_machine = dict(enumerate(sorted(machine)))\n",
        "  machine = {v:k for k,v in inv_machine.items()}\n",
        "\n",
        "  return dataset, human, machine, inv_machine"
      ],
      "metadata": {
        "id": "-sEYApqHbMrR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, human_vocab, machine_vocab, inv_machine_vocab = generate_dataset(10000)\n",
        "dataset[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sasekzTOfDNy",
        "outputId": "2b46d2de-3f10-4838-e61a-7d3175520eb2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:01<00:00, 5154.13it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('21 feb 1993', '1993-02-21'),\n",
              " ('tuesday november 20 1984', '1984-11-20'),\n",
              " ('friday april 15 2022', '2022-04-15'),\n",
              " ('11 06 12', '2012-06-11'),\n",
              " ('thursday april 19 1973', '1973-04-19'),\n",
              " ('9 apr 1971', '1971-04-09'),\n",
              " ('wednesday january 27 1982', '1982-01-27'),\n",
              " ('tuesday june 14 1994', '1994-06-14'),\n",
              " ('saturday june 16 2012', '2012-06-16'),\n",
              " ('monday february 8 2016', '2016-02-08')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here we implent text encoding manually, but we also could use tf.keras.layers.TextVectorization\n",
        "\n",
        "def encode(expression : str, Tx : int, v : dict):\n",
        "\n",
        "  #expression = expression.replace(',', '')\n",
        "  expression = expression.lower() if len(expression) <= Tx \\\n",
        "      else expression.lower()[:Tx]\n",
        "  encoding = list(map(lambda x: v.get(x, '<unk>'), expression)) # encoding or <unk> if not found\n",
        "  if len(expression) < Tx:\n",
        "    encoding += [v['<pad>']] * (Tx - len(expression))\n",
        "\n",
        "  return encoding\n",
        "\n",
        "\n",
        "def decode(encoding : list, inverse_v : dict):\n",
        "  \n",
        "  return [inverse_v[i] for i in encoding]"
      ],
      "metadata": {
        "id": "kvrdsAsdo9W5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tx, Ty = 30, 10\n",
        "\n",
        "X, Y = zip(*dataset)\n",
        "X = np.array([encode(x, Tx, human_vocab) for x in X])\n",
        "Y = np.array([encode(y, Ty, machine_vocab) for y in Y])\n",
        "\n",
        "X_onehot = np.array(list(map(lambda x: \\\n",
        "    tf.keras.utils.to_categorical(x, num_classes=len(human_vocab)), X)))\n",
        "Y_onehot = np.array(list(map(lambda y: \\\n",
        "    tf.keras.utils.to_categorical(y, num_classes=len(machine_vocab)), Y)))\n",
        "\n",
        "print(\"X.shape:\", X.shape)\n",
        "print(\"Y.shape:\", Y.shape)\n",
        "print(\"X_onehot.shape:\", X_onehot.shape)\n",
        "print(\"Y_onehot.shape:\", Y_onehot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyj5aXbRlKzr",
        "outputId": "a7212d1c-187c-44cf-f415-19e8dc49043a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape: (10000, 30)\n",
            "Y.shape: (10000, 10)\n",
            "X_onehot.shape: (10000, 30, 37)\n",
            "Y_onehot.shape: (10000, 10, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Model\n",
        "\n",
        "\n",
        "![image](https://i.imgur.com/unr4XIk.png)"
      ],
      "metadata": {
        "id": "43MPFb8W-XI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(Tx : int, Ty : int, n_a : int, n_s : int,\n",
        "                 human_vocab_size : int, machine_vocab_size : int):\n",
        "\n",
        "  # (num_samples, features) --> (num_samples, n, features)\n",
        "  repeat_vector = tf.keras.layers.RepeatVector(n=Tx)\n",
        "  concatenate = tf.keras.layers.Concatenate(axis=-1) # feature axis\n",
        "  dense1 = tf.keras.layers.Dense(10, activation='tanh')\n",
        "  dense2 = tf.keras.layers.Dense(1, activation='relu')\n",
        "  activation = tf.keras.layers.Activation(activation=tf.nn.softmax, name='attention_weights')\n",
        "  dot = tf.keras.layers.Dot(axes=1)\n",
        "\n",
        "  def attention(a, s_prev):\n",
        "    \"\"\"\n",
        "    Implements the attention block within neural net\n",
        "\n",
        "    Arguments:\n",
        "    a -- pre-attention Bi-LSTM hidden states (m, Tx, 2*n_a)\n",
        "    s_prev -- previous post-attention LSTM hidden state (m, n_s)\n",
        "\n",
        "    Returns:\n",
        "    context -- input vector for post-attention LSTM\n",
        "    \"\"\"\n",
        "    s_prev = repeat_vector(s_prev) # (m, Tx, n_s)\n",
        "    concatenated = concatenate([a, s_prev]) # (m, Tx, 2n_a + n_s)\n",
        "    energies = dense1(concatenated) # (m, Tx, 10)\n",
        "    energies = dense2(energies) # (m, Tx, 1)\n",
        "    weights = activation(energies) # (m, Tx, 1)\n",
        "    context = dot([weights, a]) # (m, 2n_a)\n",
        "\n",
        "    return context\n",
        "\n",
        "  post_attention_LSTM = tf.keras.layers.LSTM(n_s, return_state=True)\n",
        "  output_layer = tf.keras.layers.Dense(machine_vocab_size, activation='softmax')\n",
        "  outputs = []\n",
        "\n",
        "  x = tf.keras.layers.Input(shape=(Tx, human_vocab_size), name='x')\n",
        "  s0 = tf.keras.layers.Input(shape=(n_s,), name='s0')\n",
        "  c0 = tf.keras.layers.Input(shape=(n_s,), name='c0')\n",
        "  s = s0\n",
        "  c = c0\n",
        "\n",
        "  LSTM = tf.keras.layers.LSTM(n_a, return_sequences=True)\n",
        "  a = tf.keras.layers.Bidirectional(layer=LSTM, merge_mode='concat')(x)\n",
        "\n",
        "  for t in range(Ty):\n",
        "\n",
        "    context = attention(a, s)\n",
        "    s, _, c = post_attention_LSTM(context, initial_state=[s, c])\n",
        "    # here s - output (usually passed to dense layer), blank one is the last hidden_states\n",
        "    # and c is the last memory state. s[:, -1, :] (in case return_sequences = True) == last hidden_state, But since \n",
        "    # return_sequences is set to False by default, s is 2-dimensional and s == last hidden_state\n",
        "    output = output_layer(s)\n",
        "    outputs.append(output)\n",
        "\n",
        "  return tf.keras.Model(inputs=[x, s0, c0], outputs=outputs)"
      ],
      "metadata": {
        "id": "ku2uWlEf8KLH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_a = 32 # pre-attention hidden states\n",
        "n_s = 64 # post-attention hidden states\n",
        "\n",
        "model = create_model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvOe-Vs1xt7P",
        "outputId": "d24043ec-0872-4183-fb55-0b0d4b74a86e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " x (InputLayer)                 [(None, 30, 37)]     0           []                               \n",
            "                                                                                                  \n",
            " s0 (InputLayer)                [(None, 64)]         0           []                               \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirectional  (None, 30, 64)      17920       ['x[0][0]']                      \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " repeat_vector_2 (RepeatVector)  (None, 30, 64)      0           ['s0[0][0]',                     \n",
            "                                                                  'lstm_4[0][0]',                 \n",
            "                                                                  'lstm_4[1][0]',                 \n",
            "                                                                  'lstm_4[2][0]',                 \n",
            "                                                                  'lstm_4[3][0]',                 \n",
            "                                                                  'lstm_4[4][0]',                 \n",
            "                                                                  'lstm_4[5][0]',                 \n",
            "                                                                  'lstm_4[6][0]',                 \n",
            "                                                                  'lstm_4[7][0]',                 \n",
            "                                                                  'lstm_4[8][0]']                 \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 30, 128)      0           ['bidirectional_2[0][0]',        \n",
            "                                                                  'repeat_vector_2[0][0]',        \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'repeat_vector_2[1][0]',        \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'repeat_vector_2[2][0]',        \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'repeat_vector_2[3][0]',        \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'repeat_vector_2[4][0]',        \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'repeat_vector_2[5][0]',        \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'repeat_vector_2[6][0]',        \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'repeat_vector_2[7][0]',        \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'repeat_vector_2[8][0]',        \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'repeat_vector_2[9][0]']        \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 30, 10)       1290        ['concatenate_2[0][0]',          \n",
            "                                                                  'concatenate_2[1][0]',          \n",
            "                                                                  'concatenate_2[2][0]',          \n",
            "                                                                  'concatenate_2[3][0]',          \n",
            "                                                                  'concatenate_2[4][0]',          \n",
            "                                                                  'concatenate_2[5][0]',          \n",
            "                                                                  'concatenate_2[6][0]',          \n",
            "                                                                  'concatenate_2[7][0]',          \n",
            "                                                                  'concatenate_2[8][0]',          \n",
            "                                                                  'concatenate_2[9][0]']          \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 30, 1)        11          ['dense_6[0][0]',                \n",
            "                                                                  'dense_6[1][0]',                \n",
            "                                                                  'dense_6[2][0]',                \n",
            "                                                                  'dense_6[3][0]',                \n",
            "                                                                  'dense_6[4][0]',                \n",
            "                                                                  'dense_6[5][0]',                \n",
            "                                                                  'dense_6[6][0]',                \n",
            "                                                                  'dense_6[7][0]',                \n",
            "                                                                  'dense_6[8][0]',                \n",
            "                                                                  'dense_6[9][0]']                \n",
            "                                                                                                  \n",
            " attention_weights (Activation)  (None, 30, 1)       0           ['dense_7[0][0]',                \n",
            "                                                                  'dense_7[1][0]',                \n",
            "                                                                  'dense_7[2][0]',                \n",
            "                                                                  'dense_7[3][0]',                \n",
            "                                                                  'dense_7[4][0]',                \n",
            "                                                                  'dense_7[5][0]',                \n",
            "                                                                  'dense_7[6][0]',                \n",
            "                                                                  'dense_7[7][0]',                \n",
            "                                                                  'dense_7[8][0]',                \n",
            "                                                                  'dense_7[9][0]']                \n",
            "                                                                                                  \n",
            " dot_2 (Dot)                    (None, 1, 64)        0           ['attention_weights[0][0]',      \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'attention_weights[1][0]',      \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'attention_weights[2][0]',      \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'attention_weights[3][0]',      \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'attention_weights[4][0]',      \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'attention_weights[5][0]',      \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'attention_weights[6][0]',      \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'attention_weights[7][0]',      \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'attention_weights[8][0]',      \n",
            "                                                                  'bidirectional_2[0][0]',        \n",
            "                                                                  'attention_weights[9][0]',      \n",
            "                                                                  'bidirectional_2[0][0]']        \n",
            "                                                                                                  \n",
            " c0 (InputLayer)                [(None, 64)]         0           []                               \n",
            "                                                                                                  \n",
            " lstm_4 (LSTM)                  [(None, 64),         33024       ['dot_2[0][0]',                  \n",
            "                                 (None, 64),                      's0[0][0]',                     \n",
            "                                 (None, 64)]                      'c0[0][0]',                     \n",
            "                                                                  'dot_2[1][0]',                  \n",
            "                                                                  'lstm_4[0][0]',                 \n",
            "                                                                  'lstm_4[0][2]',                 \n",
            "                                                                  'dot_2[2][0]',                  \n",
            "                                                                  'lstm_4[1][0]',                 \n",
            "                                                                  'lstm_4[1][2]',                 \n",
            "                                                                  'dot_2[3][0]',                  \n",
            "                                                                  'lstm_4[2][0]',                 \n",
            "                                                                  'lstm_4[2][2]',                 \n",
            "                                                                  'dot_2[4][0]',                  \n",
            "                                                                  'lstm_4[3][0]',                 \n",
            "                                                                  'lstm_4[3][2]',                 \n",
            "                                                                  'dot_2[5][0]',                  \n",
            "                                                                  'lstm_4[4][0]',                 \n",
            "                                                                  'lstm_4[4][2]',                 \n",
            "                                                                  'dot_2[6][0]',                  \n",
            "                                                                  'lstm_4[5][0]',                 \n",
            "                                                                  'lstm_4[5][2]',                 \n",
            "                                                                  'dot_2[7][0]',                  \n",
            "                                                                  'lstm_4[6][0]',                 \n",
            "                                                                  'lstm_4[6][2]',                 \n",
            "                                                                  'dot_2[8][0]',                  \n",
            "                                                                  'lstm_4[7][0]',                 \n",
            "                                                                  'lstm_4[7][2]',                 \n",
            "                                                                  'dot_2[9][0]',                  \n",
            "                                                                  'lstm_4[8][0]',                 \n",
            "                                                                  'lstm_4[8][2]']                 \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 11)           715         ['lstm_4[0][0]',                 \n",
            "                                                                  'lstm_4[1][0]',                 \n",
            "                                                                  'lstm_4[2][0]',                 \n",
            "                                                                  'lstm_4[3][0]',                 \n",
            "                                                                  'lstm_4[4][0]',                 \n",
            "                                                                  'lstm_4[5][0]',                 \n",
            "                                                                  'lstm_4[6][0]',                 \n",
            "                                                                  'lstm_4[7][0]',                 \n",
            "                                                                  'lstm_4[8][0]',                 \n",
            "                                                                  'lstm_4[9][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 52,960\n",
            "Trainable params: 52,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=.005, decay=.01),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['acc']\n",
        ")"
      ],
      "metadata": {
        "id": "6UbxLOD1yEn9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.output_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3-QPPh905Gd",
        "outputId": "1bc10e26-6230-4104-96d8-185e630946ef"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(None, 11),\n",
              " (None, 11),\n",
              " (None, 11),\n",
              " (None, 11),\n",
              " (None, 11),\n",
              " (None, 11),\n",
              " (None, 11),\n",
              " (None, 11),\n",
              " (None, 11),\n",
              " (None, 11)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = 10000\n",
        "batch_size = 100\n",
        "\n",
        "s0 = np.zeros((m, n_s)) # fit() will batch them into (batch_size, n_s)\n",
        "c0 = np.zeros((m, n_s))\n",
        "\n",
        "outputs = list(Y_onehot.swapaxes(0, 1))\n",
        "len(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5-mFMGf2_Ht",
        "outputId": "b9f065c4-4bac-48c1-d4d0-f5f1dd822447"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    x=[X_onehot, s0, c0],\n",
        "    y=outputs,\n",
        "    epochs=10,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLO1VMTK3e30",
        "outputId": "f4eebf69-b797-4120-ee24-19eb9c7100f7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "313/313 [==============================] - 44s 54ms/step - loss: 11.7234 - dense_8_loss: 0.4227 - dense_8_1_loss: 0.4355 - dense_8_2_loss: 1.0907 - dense_8_3_loss: 1.9952 - dense_8_4_loss: 0.6270 - dense_8_5_loss: 1.0300 - dense_8_6_loss: 1.9893 - dense_8_7_loss: 0.7091 - dense_8_8_loss: 1.3917 - dense_8_9_loss: 2.0323 - dense_8_acc: 0.9021 - dense_8_1_acc: 0.8828 - dense_8_2_acc: 0.6212 - dense_8_3_acc: 0.2708 - dense_8_4_acc: 0.8508 - dense_8_5_acc: 0.5521 - dense_8_6_acc: 0.2545 - dense_8_7_acc: 0.8051 - dense_8_8_acc: 0.4076 - dense_8_9_acc: 0.2514\n",
            "Epoch 2/10\n",
            "313/313 [==============================] - 19s 61ms/step - loss: 5.4371 - dense_8_loss: 0.1222 - dense_8_1_loss: 0.0908 - dense_8_2_loss: 0.5597 - dense_8_3_loss: 1.2553 - dense_8_4_loss: 0.0194 - dense_8_5_loss: 0.2524 - dense_8_6_loss: 1.1204 - dense_8_7_loss: 0.0239 - dense_8_8_loss: 0.7019 - dense_8_9_loss: 1.2910 - dense_8_acc: 0.9662 - dense_8_1_acc: 0.9670 - dense_8_2_acc: 0.7926 - dense_8_3_acc: 0.5244 - dense_8_4_acc: 1.0000 - dense_8_5_acc: 0.9332 - dense_8_6_acc: 0.5951 - dense_8_7_acc: 0.9990 - dense_8_8_acc: 0.7390 - dense_8_9_acc: 0.4854\n",
            "Epoch 3/10\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 4.0155 - dense_8_loss: 0.0936 - dense_8_1_loss: 0.0738 - dense_8_2_loss: 0.4702 - dense_8_3_loss: 0.8740 - dense_8_4_loss: 0.0144 - dense_8_5_loss: 0.1763 - dense_8_6_loss: 0.8233 - dense_8_7_loss: 0.0158 - dense_8_8_loss: 0.5210 - dense_8_9_loss: 0.9530 - dense_8_acc: 0.9702 - dense_8_1_acc: 0.9727 - dense_8_2_acc: 0.8187 - dense_8_3_acc: 0.7135 - dense_8_4_acc: 1.0000 - dense_8_5_acc: 0.9521 - dense_8_6_acc: 0.7350 - dense_8_7_acc: 0.9998 - dense_8_8_acc: 0.8154 - dense_8_9_acc: 0.6483\n",
            "Epoch 4/10\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 3.1548 - dense_8_loss: 0.0807 - dense_8_1_loss: 0.0660 - dense_8_2_loss: 0.4274 - dense_8_3_loss: 0.6121 - dense_8_4_loss: 0.0122 - dense_8_5_loss: 0.1374 - dense_8_6_loss: 0.6430 - dense_8_7_loss: 0.0117 - dense_8_8_loss: 0.4225 - dense_8_9_loss: 0.7417 - dense_8_acc: 0.9722 - dense_8_1_acc: 0.9738 - dense_8_2_acc: 0.8261 - dense_8_3_acc: 0.8293 - dense_8_4_acc: 1.0000 - dense_8_5_acc: 0.9628 - dense_8_6_acc: 0.8095 - dense_8_7_acc: 1.0000 - dense_8_8_acc: 0.8594 - dense_8_9_acc: 0.7446\n",
            "Epoch 5/10\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 2.6412 - dense_8_loss: 0.0722 - dense_8_1_loss: 0.0608 - dense_8_2_loss: 0.3981 - dense_8_3_loss: 0.4732 - dense_8_4_loss: 0.0096 - dense_8_5_loss: 0.1189 - dense_8_6_loss: 0.5310 - dense_8_7_loss: 0.0092 - dense_8_8_loss: 0.3540 - dense_8_9_loss: 0.6143 - dense_8_acc: 0.9738 - dense_8_1_acc: 0.9759 - dense_8_2_acc: 0.8262 - dense_8_3_acc: 0.8733 - dense_8_4_acc: 1.0000 - dense_8_5_acc: 0.9680 - dense_8_6_acc: 0.8536 - dense_8_7_acc: 1.0000 - dense_8_8_acc: 0.8867 - dense_8_9_acc: 0.7905\n",
            "Epoch 6/10\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 2.3083 - dense_8_loss: 0.0673 - dense_8_1_loss: 0.0575 - dense_8_2_loss: 0.3785 - dense_8_3_loss: 0.3862 - dense_8_4_loss: 0.0080 - dense_8_5_loss: 0.1042 - dense_8_6_loss: 0.4658 - dense_8_7_loss: 0.0079 - dense_8_8_loss: 0.3060 - dense_8_9_loss: 0.5269 - dense_8_acc: 0.9744 - dense_8_1_acc: 0.9762 - dense_8_2_acc: 0.8264 - dense_8_3_acc: 0.9004 - dense_8_4_acc: 1.0000 - dense_8_5_acc: 0.9724 - dense_8_6_acc: 0.8718 - dense_8_7_acc: 0.9999 - dense_8_8_acc: 0.9042 - dense_8_9_acc: 0.8287\n",
            "Epoch 7/10\n",
            "313/313 [==============================] - 18s 56ms/step - loss: 2.0791 - dense_8_loss: 0.0634 - dense_8_1_loss: 0.0547 - dense_8_2_loss: 0.3611 - dense_8_3_loss: 0.3360 - dense_8_4_loss: 0.0068 - dense_8_5_loss: 0.0956 - dense_8_6_loss: 0.4170 - dense_8_7_loss: 0.0070 - dense_8_8_loss: 0.2678 - dense_8_9_loss: 0.4696 - dense_8_acc: 0.9757 - dense_8_1_acc: 0.9768 - dense_8_2_acc: 0.8262 - dense_8_3_acc: 0.9142 - dense_8_4_acc: 1.0000 - dense_8_5_acc: 0.9748 - dense_8_6_acc: 0.8865 - dense_8_7_acc: 1.0000 - dense_8_8_acc: 0.9155 - dense_8_9_acc: 0.8487\n",
            "Epoch 8/10\n",
            "313/313 [==============================] - 18s 57ms/step - loss: 1.9123 - dense_8_loss: 0.0603 - dense_8_1_loss: 0.0526 - dense_8_2_loss: 0.3461 - dense_8_3_loss: 0.2974 - dense_8_4_loss: 0.0059 - dense_8_5_loss: 0.0890 - dense_8_6_loss: 0.3857 - dense_8_7_loss: 0.0065 - dense_8_8_loss: 0.2424 - dense_8_9_loss: 0.4263 - dense_8_acc: 0.9761 - dense_8_1_acc: 0.9771 - dense_8_2_acc: 0.8295 - dense_8_3_acc: 0.9257 - dense_8_4_acc: 1.0000 - dense_8_5_acc: 0.9764 - dense_8_6_acc: 0.8915 - dense_8_7_acc: 1.0000 - dense_8_8_acc: 0.9259 - dense_8_9_acc: 0.8640\n",
            "Epoch 9/10\n",
            "313/313 [==============================] - 19s 60ms/step - loss: 1.7788 - dense_8_loss: 0.0575 - dense_8_1_loss: 0.0497 - dense_8_2_loss: 0.3335 - dense_8_3_loss: 0.2704 - dense_8_4_loss: 0.0052 - dense_8_5_loss: 0.0831 - dense_8_6_loss: 0.3576 - dense_8_7_loss: 0.0060 - dense_8_8_loss: 0.2230 - dense_8_9_loss: 0.3929 - dense_8_acc: 0.9769 - dense_8_1_acc: 0.9785 - dense_8_2_acc: 0.8324 - dense_8_3_acc: 0.9362 - dense_8_4_acc: 1.0000 - dense_8_5_acc: 0.9780 - dense_8_6_acc: 0.8972 - dense_8_7_acc: 1.0000 - dense_8_8_acc: 0.9300 - dense_8_9_acc: 0.8716\n",
            "Epoch 10/10\n",
            "313/313 [==============================] - 17s 56ms/step - loss: 1.6789 - dense_8_loss: 0.0552 - dense_8_1_loss: 0.0482 - dense_8_2_loss: 0.3242 - dense_8_3_loss: 0.2499 - dense_8_4_loss: 0.0045 - dense_8_5_loss: 0.0792 - dense_8_6_loss: 0.3377 - dense_8_7_loss: 0.0055 - dense_8_8_loss: 0.2054 - dense_8_9_loss: 0.3691 - dense_8_acc: 0.9777 - dense_8_1_acc: 0.9791 - dense_8_2_acc: 0.8342 - dense_8_3_acc: 0.9417 - dense_8_4_acc: 1.0000 - dense_8_5_acc: 0.9786 - dense_8_6_acc: 0.9029 - dense_8_7_acc: 1.0000 - dense_8_8_acc: 0.9347 - dense_8_9_acc: 0.8783\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe763bee490>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating"
      ],
      "metadata": {
        "id": "v4l8wncy6mRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, human_vocab, machine_vocab, inv_machine_vocab = generate_dataset(1000)\n",
        "dataset = dataset[:10]\n",
        "\n",
        "X, Y = zip(*dataset)\n",
        "X = np.array([encode(x, Tx, human_vocab) for x in X])\n",
        "Y = np.array([encode(y, Ty, machine_vocab) for y in Y])\n",
        "\n",
        "X_onehot = np.array(list(map(lambda x: \\\n",
        "    tf.keras.utils.to_categorical(x, num_classes=len(human_vocab)), X)))\n",
        "Y_onehot = np.array(list(map(lambda y: \\\n",
        "    tf.keras.utils.to_categorical(y, num_classes=len(machine_vocab)), Y)))\n",
        "\n",
        "print(\"X.shape:\", X.shape)\n",
        "print(\"Y.shape:\", Y.shape)\n",
        "print(\"X_onehot.shape:\", X_onehot.shape)\n",
        "print(\"Y_onehot.shape:\", Y_onehot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BerNPfKy6lAQ",
        "outputId": "221eb679-a21d-450a-90d6-088d112c33f4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 4463.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape: (10, 30)\n",
            "Y.shape: (10, 10)\n",
            "X_onehot.shape: (10, 30, 37)\n",
            "Y_onehot.shape: (10, 10, 11)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict([X_onehot, np.zeros((10, n_s)), np.zeros((10, n_s))])\n",
        "predictions = np.argmax(np.stack(predictions), axis=-1)\n",
        "predictions.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfpcwGWw7Dyf",
        "outputId": "d59c1568-1498-413b-f53a-3a8f70d65f70"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, _ = zip(*dataset)\n",
        "\n",
        "for x, predicted in zip(X, list(predictions.T)):\n",
        "  print('{0:11}'.format('x :'), x)\n",
        "  output = [inv_machine_vocab[int(p)] for p in predicted]\n",
        "  print('predicted :', ''.join(output), '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWzUUbAs8-1-",
        "outputId": "446dc689-35a3-48f0-fd0a-87de70471afc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x :         23 mar 1986\n",
            "predicted : 1986-03-23 \n",
            "\n",
            "x :         saturday september 21 1996\n",
            "predicted : 1996-09-21 \n",
            "\n",
            "x :         thursday july 22 1971\n",
            "predicted : 1971-07-22 \n",
            "\n",
            "x :         august 7 2021\n",
            "predicted : 2011-08-07 \n",
            "\n",
            "x :         saturday december 26 2015\n",
            "predicted : 2015-12-26 \n",
            "\n",
            "x :         tuesday october 23 2012\n",
            "predicted : 2012-10-23 \n",
            "\n",
            "x :         thursday february 27 1975\n",
            "predicted : 1975-02-27 \n",
            "\n",
            "x :         23 december 1975\n",
            "predicted : 1975-12-23 \n",
            "\n",
            "x :         1 10 94\n",
            "predicted : 1994-01-01 \n",
            "\n",
            "x :         tuesday january 17 1995\n",
            "predicted : 1995-01-17 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention maps"
      ],
      "metadata": {
        "id": "Y_oJexyyDMix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_attention_map(model : tf.keras.Model, input_vocabulary : dict,\n",
        "                       inv_output_vocabulary : dict, text : str,\n",
        "                       n_s : int = 128, num : int = 7):\n",
        "\n",
        "    attention_map = np.zeros((10, 30))\n",
        "    layer = model.get_layer('attention_weights')\n",
        "\n",
        "    Ty, Tx = attention_map.shape\n",
        "    \n",
        "    human_vocab_size = 37\n",
        "    \n",
        "    # Well, this is cumbersome but this version of tensorflow-keras has a bug that affects the \n",
        "    # reuse of layers in a model with the functional API. \n",
        "    # So, I have to recreate the model based on the functional \n",
        "    # components and connect then one by one.\n",
        "    # ideally it can be done simply like this:\n",
        "    # layer = modelx.layers[num]\n",
        "    # f = Model(modelx.inputs, [layer.get_output_at(t) for t in range(Ty)])\n",
        "    \n",
        "    X = model.inputs[0] \n",
        "    s0 = model.inputs[1] \n",
        "    c0 = model.inputs[2] \n",
        "    s = s0\n",
        "    c = s0\n",
        "    \n",
        "    a = model.layers[2](X)  \n",
        "    outputs = []\n",
        "\n",
        "    for t in range(Ty):\n",
        "        s_prev = s\n",
        "        s_prev = model.layers[3](s_prev)\n",
        "        concat = model.layers[4]([a, s_prev]) \n",
        "        e = model.layers[5](concat) \n",
        "        energies = model.layers[6](e) \n",
        "        alphas = model.layers[7](energies) \n",
        "        context = model.layers[8]([alphas, a])\n",
        "        s, _, c = model.layers[10](context, initial_state = [s, c]) \n",
        "        outputs.append(energies)\n",
        "\n",
        "    f = tf.keras.Model(inputs=[X, s0, c0], outputs = outputs)\n",
        "    \n",
        "\n",
        "    s0 = np.zeros((1, n_s))\n",
        "    c0 = np.zeros((1, n_s))\n",
        "    encoded = np.array(encode(text, Tx, input_vocabulary)).reshape((1, 30))\n",
        "    encoded = np.array(list(map(lambda x: tf.keras.utils.to_categorical(x, num_classes=len(input_vocabulary)), encoded)))\n",
        "\n",
        "    \n",
        "    r = f([encoded, s0, c0])\n",
        "        \n",
        "    for t in range(Ty):\n",
        "        for t_prime in range(Tx):\n",
        "            attention_map[t][t_prime] = r[t][0, t_prime]\n",
        "\n",
        "    # Normalize attention map\n",
        "    row_max = attention_map.max(axis=1)\n",
        "    attention_map = attention_map / row_max[:, None]\n",
        "\n",
        "    prediction = model.predict([encoded, s0, c0])\n",
        "    \n",
        "    predicted_text = []\n",
        "    for i in range(len(prediction)):\n",
        "        predicted_text.append(int(np.argmax(prediction[i], axis=1)))\n",
        "        \n",
        "    predicted_text = list(predicted_text)\n",
        "    predicted_text = decode(predicted_text, inv_output_vocabulary)\n",
        "    text_ = list(text)\n",
        "    \n",
        "    # get the lengths of the string\n",
        "    input_length = len(text)\n",
        "    output_length = Ty\n",
        "    \n",
        "    # Plot the attention_map\n",
        "    plt.clf()\n",
        "    f = plt.figure(figsize=(8, 8.5))\n",
        "    ax = f.add_subplot(1, 1, 1)\n",
        "\n",
        "    # add image\n",
        "    i = ax.imshow(attention_map, interpolation='nearest', cmap='Blues')\n",
        "\n",
        "    # add colorbar\n",
        "    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])\n",
        "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
        "    cbar.ax.set_xlabel('Alpha value (Probability output of the \"softmax\")', labelpad=2)\n",
        "\n",
        "    # add labels\n",
        "    ax.set_yticks(range(output_length))\n",
        "    ax.set_yticklabels(predicted_text[:output_length])\n",
        "\n",
        "    ax.set_xticks(range(input_length))\n",
        "    ax.set_xticklabels(text_[:input_length], rotation=45)\n",
        "\n",
        "    ax.set_xlabel('Input Sequence')\n",
        "    ax.set_ylabel('Output Sequence')\n",
        "\n",
        "    # add grid and legend\n",
        "    ax.grid()\n",
        "\n",
        "    #f.show()\n",
        "    \n",
        "    return attention_map"
      ],
      "metadata": {
        "id": "5ghadOhvxQHC"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab,\n",
        "                                   \"tuesday january 17 1995\", n_s = 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "5_5msBk4F6BU",
        "outputId": "861a8269-40f1-47d0-aa1f-dbe8c2f9073f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x612 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}