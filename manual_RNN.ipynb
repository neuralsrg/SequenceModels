{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "manual_RNN.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOYAyqGchJ8SjNoOBQYuDu+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuralsrg/SequenceModels/blob/main/manual_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual implementation of RNN in numpy"
      ],
      "metadata": {
        "id": "NWNdRPg_rnkr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "U2P7n0bJrkt1"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN"
      ],
      "metadata": {
        "id": "PBMqDwlMGzPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN unit\n",
        "![image](https://i.imgur.com/7PAU2pP.png)"
      ],
      "metadata": {
        "id": "zqBTQ5k5r9lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_unit(x, a_prev, parameters):\n",
        "  '''\n",
        "  Implements signle RNN unit forward pass\n",
        "\n",
        "  Args:\n",
        "  x -- input vector\n",
        "  a_prev -- activation from previous RNN unit\n",
        "  parameters -- python dictionary with keys 'Waa', 'Wax', 'ba', 'Wya', 'by'\n",
        "\n",
        "  Returns:\n",
        "  y_pred -- softmax activation outputs\n",
        "  a -- current RNN unit hidden state\n",
        "  cache -- tuple containing (a, a_prev, x, parameters)\n",
        "  '''\n",
        "  Waa, Wax, ba, Wya, by = parameters['Waa'], parameters['Wax'], parameters['ba'], \\\n",
        "                          parameters['Wya'], parameters['by']\n",
        "\n",
        "  a = Waa @ a_prev + Wax @ x + ba\n",
        "  a = np.tanh(a)\n",
        "\n",
        "  y_pred = scipy.special.softmax(Wya @ a + by, axis=0)\n",
        "\n",
        "  cache = (a, a_prev, x, parameters)\n",
        "\n",
        "  return y_pred, a, cache"
      ],
      "metadata": {
        "id": "l5qDDOfer0Hq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN forward pass"
      ],
      "metadata": {
        "id": "AzzwyTv1ze07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn(x, a_init, parameters):\n",
        "  '''\n",
        "  Implements RNN forward pass\n",
        "\n",
        "  Args:\n",
        "  x -- tensor of shape (n_x, m, T_x), where n_x - num of features, m - batch size,\n",
        "      T_x - num of time steps\n",
        "  a_init -- initial rnn hidden state\n",
        "  parameters -- python dictionary with keys 'Waa', 'Wax', 'ba', 'Wya', 'by'\n",
        "\n",
        "  Returns:\n",
        "  Y_pred -- tensor of shape (n_y, m, T_x)\n",
        "  A -- tensor of hidden states of shape (n_a, m, T_x)\n",
        "  caches -- (list_of_caches, x)\n",
        "  '''\n",
        "  n_x, m, T_x = x.shape\n",
        "  a = a_init\n",
        "  n_y, n_a = parameters['Wya'].shape\n",
        "\n",
        "  Y_pred = np.empty((n_y, m, 0))\n",
        "  A = np.empty((n_a, m, 0))\n",
        "  caches = []\n",
        "\n",
        "  for time_step in range(T_x):\n",
        "    y_pred, a, cache = rnn_unit(x[:, :, time_step], a, parameters)\n",
        "\n",
        "    Y_pred = np.append(Y_pred, y_pred[..., np.newaxis], axis=-1)\n",
        "    A = np.append(A, a[..., np.newaxis], axis=-1)\n",
        "    caches.append(cache)\n",
        "\n",
        "  return Y_pred, A, (caches, x)"
      ],
      "metadata": {
        "id": "bUZy2eVtx1tN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN backpropagation\n",
        "\n",
        "![image](https://i.imgur.com/x2jxcSq.png)"
      ],
      "metadata": {
        "id": "xREIaG1pG3uE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{align}\n",
        "\\displaystyle a^{\\langle t \\rangle} &= \\tanh(W_{ax} x^{\\langle t \\rangle} + W_{aa} a^{\\langle t-1 \\rangle} + b_{a})\\tag{-} \\\\[8pt]\n",
        "\\displaystyle \\frac{\\partial \\tanh(x)} {\\partial x} &= 1 - \\tanh^2(x) \\tag{-} \\\\[8pt]\n",
        "\\displaystyle {dtanh} &= da_{next} * ( 1 - \\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a})) \\tag{0} \\\\[8pt]\n",
        "\\displaystyle  {dW_{ax}} &= dtanh \\cdot x^{\\langle t \\rangle T}\\tag{1} \\\\[8pt]\n",
        "\\displaystyle dW_{aa} &= dtanh \\cdot a^{\\langle t-1 \\rangle T}\\tag{2} \\\\[8pt]\n",
        "\\displaystyle db_a& = \\sum_{batch}dtanh\\tag{3} \\\\[8pt]\n",
        "\\displaystyle dx^{\\langle t \\rangle} &= { W_{ax}}^T \\cdot dtanh\\tag{4} \\\\[8pt]\n",
        "\\displaystyle da_{prev} &= { W_{aa}}^T \\cdot dtanh\\tag{5}\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "pMRByF_vFUUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_unit_backward(da, cache):\n",
        "  '''\n",
        "  Implements the backward pass for a single rnn unit based on it's cache\n",
        "\n",
        "  Args:\n",
        "  da -- dJ/da (da_next from the picture above)\n",
        "  cache -- cache from rnn forward pass\n",
        "\n",
        "  Returns:\n",
        "  grads -- python dictionary with keys: 'dba', 'dWax', 'dx' (for deep RNNs),\n",
        "      'dWaa', 'dWa_prev', 'da_prev'\n",
        "  '''\n",
        "  a, a_prev, x, parameters = cache\n",
        "  Waa, Wax, ba, Wya, by = parameters['Waa'], parameters['Wax'], parameters['ba'], \\\n",
        "                          parameters['Wya'], parameters['by']\n",
        "\n",
        "  grads = {}\n",
        "\n",
        "  dz = da * (1 - np.square(a))\n",
        "\n",
        "  grads['dba'] = np.sum(dz, axis=-1, keepdims=True)\n",
        "  grads['dWaa'] = dz @ a_prev.T # it is basically a sum over mini-batch sample gradients\n",
        "  grads['da_prev'] = Waa.T @ dz # shape (n_a, m)\n",
        "\n",
        "  grads['dWax'] = dz @ x.T\n",
        "  grads['dx'] = Wax.T @ dz\n",
        "\n",
        "  return grads"
      ],
      "metadata": {
        "id": "I7nLz-upEfaP"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_backward(da, caches):\n",
        "  '''\n",
        "  Implements the backward pass for the entire RNN\n",
        "\n",
        "  Args:\n",
        "  da -- dJ/da of shape (n_a, m, T_x) computed elsewhere\n",
        "  caches -- output from rnn()\n",
        "\n",
        "  Returns:\n",
        "  grads -- python dictionary with keys \n",
        "    'dx' : shape(n_x, m, T_x)\n",
        "    'da_init' : shape(n_a, m)\n",
        "    'dWax' : shape(n_a, n_x)\n",
        "    'dWaa' : shape(n_a, n_a)\n",
        "    'dba' : shape(n_a, 1)\n",
        "  '''\n",
        "  caches, x = caches\n",
        "  a, a_init, x, parameters = caches[0]\n",
        "\n",
        "  n_a, m, T_x = da.shape\n",
        "  n_x, _ = x.shape\n",
        "\n",
        "  dx = np.empty((n_x, m, 0))\n",
        "  dWax = np.zeros((n_a, n_x))\n",
        "  dWaa = np.zeros((n_a, n_a))\n",
        "  dba = np.zeros((n_a, 1))\n",
        "\n",
        "  # to save the last value\n",
        "  da_prev = np.zeros((n_a, m))\n",
        "\n",
        "  for time_step in range(T_x - 1, -1, -1):\n",
        "    grads = rnn_unit_backward(da[:, :, time_step] + da_prev, caches[time_step])\n",
        "    dba_t, dWaa_t, da_prev, dWax_t, dx_t = grads['dba'], grads['dWaa'], \\\n",
        "        grads['da_prev'], grads['dWax'], grads['dx']\n",
        "    \n",
        "    dx = np.append(dx_t[..., np.newaxis], dx, axis=-1)\n",
        "    dWax += dWax_t\n",
        "    dWaa += dWaa_t\n",
        "    dba += dba_t\n",
        "  \n",
        "  grads = {\"dx\": dx, \"da0\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n",
        "\n",
        "  return grads"
      ],
      "metadata": {
        "id": "eeD0JfrHNRxT"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fbv1cQBoY7qS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}