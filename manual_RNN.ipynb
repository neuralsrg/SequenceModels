{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "manual_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN2H7Br27h47nxGfhPyA5lm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuralsrg/SequenceModels/blob/main/manual_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual implementation of RNN in numpy"
      ],
      "metadata": {
        "id": "NWNdRPg_rnkr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U2P7n0bJrkt1"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import scipy.special\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN"
      ],
      "metadata": {
        "id": "PBMqDwlMGzPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN unit\n",
        "![image](https://i.imgur.com/7PAU2pP.png)"
      ],
      "metadata": {
        "id": "zqBTQ5k5r9lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_unit(x, a_prev, parameters):\n",
        "  '''\n",
        "  Implements signle RNN unit forward pass\n",
        "\n",
        "  Args:\n",
        "  x -- input vector\n",
        "  a_prev -- activation from previous RNN unit\n",
        "  parameters -- python dictionary with keys 'Waa', 'Wax', 'ba', 'Wya', 'by'\n",
        "\n",
        "  Returns:\n",
        "  y_pred -- softmax activation outputs\n",
        "  a -- current RNN unit hidden state\n",
        "  cache -- tuple containing (a, a_prev, x, parameters)\n",
        "  '''\n",
        "  Waa, Wax, ba, Wya, by = parameters['Waa'], parameters['Wax'], parameters['ba'], \\\n",
        "                          parameters['Wya'], parameters['by']\n",
        "\n",
        "  a = Waa @ a_prev + Wax @ x + ba\n",
        "  a = np.tanh(a)\n",
        "\n",
        "  y_pred = scipy.special.softmax(Wya @ a + by, axis=0)\n",
        "\n",
        "  cache = (a, a_prev, x, parameters)\n",
        "\n",
        "  return y_pred, a, cache"
      ],
      "metadata": {
        "id": "l5qDDOfer0Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN forward pass"
      ],
      "metadata": {
        "id": "AzzwyTv1ze07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn(x, a_init, parameters):\n",
        "  '''\n",
        "  Implements RNN forward pass\n",
        "\n",
        "  Args:\n",
        "  x -- tensor of shape (n_x, m, T_x), where n_x - num of features, m - batch size,\n",
        "      T_x - num of time steps\n",
        "  a_init -- initial rnn hidden state\n",
        "  parameters -- python dictionary with keys 'Waa', 'Wax', 'ba', 'Wya', 'by'\n",
        "\n",
        "  Returns:\n",
        "  Y_pred -- tensor of shape (n_y, m, T_x)\n",
        "  A -- tensor of hidden states of shape (n_a, m, T_x)\n",
        "  caches -- (list_of_caches, x)\n",
        "  '''\n",
        "  n_x, m, T_x = x.shape\n",
        "  a = a_init\n",
        "  n_y, n_a = parameters['Wya'].shape\n",
        "\n",
        "  Y_pred = np.empty((n_y, m, 0))\n",
        "  A = np.empty((n_a, m, 0))\n",
        "  caches = []\n",
        "\n",
        "  for time_step in range(T_x):\n",
        "    y_pred, a, cache = rnn_unit(x[:, :, time_step], a, parameters)\n",
        "\n",
        "    Y_pred = np.append(Y_pred, y_pred[..., np.newaxis], axis=-1)\n",
        "    A = np.append(A, a[..., np.newaxis], axis=-1)\n",
        "    caches.append(cache)\n",
        "\n",
        "  return Y_pred, A, (caches, x)"
      ],
      "metadata": {
        "id": "bUZy2eVtx1tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN backpropagation\n",
        "\n",
        "![image](https://i.imgur.com/x2jxcSq.png)"
      ],
      "metadata": {
        "id": "xREIaG1pG3uE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{align}\n",
        "\\displaystyle a^{\\langle t \\rangle} &= \\tanh(W_{ax} x^{\\langle t \\rangle} + W_{aa} a^{\\langle t-1 \\rangle} + b_{a})\\tag{-} \\\\[8pt]\n",
        "\\displaystyle \\frac{\\partial \\tanh(x)} {\\partial x} &= 1 - \\tanh^2(x) \\tag{-} \\\\[8pt]\n",
        "\\displaystyle {dtanh} &= da_{next} * ( 1 - \\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a})) \\tag{0} \\\\[8pt]\n",
        "\\displaystyle  {dW_{ax}} &= dtanh \\cdot x^{\\langle t \\rangle T}\\tag{1} \\\\[8pt]\n",
        "\\displaystyle dW_{aa} &= dtanh \\cdot a^{\\langle t-1 \\rangle T}\\tag{2} \\\\[8pt]\n",
        "\\displaystyle db_a& = \\sum_{batch}dtanh\\tag{3} \\\\[8pt]\n",
        "\\displaystyle dx^{\\langle t \\rangle} &= { W_{ax}}^T \\cdot dtanh\\tag{4} \\\\[8pt]\n",
        "\\displaystyle da_{prev} &= { W_{aa}}^T \\cdot dtanh\\tag{5}\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "pMRByF_vFUUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_unit_backward(da, cache):\n",
        "  '''\n",
        "  Implements the backward pass for a single rnn unit based on it's cache\n",
        "\n",
        "  Args:\n",
        "  da -- dJ/da (da_next from the picture above)\n",
        "  cache -- cache from rnn forward pass\n",
        "\n",
        "  Returns:\n",
        "  grads -- python dictionary with keys: 'dba', 'dWax', 'dx' (for deep RNNs),\n",
        "      'dWaa', 'dWa_prev', 'da_prev'\n",
        "  '''\n",
        "  a, a_prev, x, parameters = cache\n",
        "  Waa, Wax, ba, Wya, by = parameters['Waa'], parameters['Wax'], parameters['ba'], \\\n",
        "                          parameters['Wya'], parameters['by']\n",
        "\n",
        "  grads = {}\n",
        "\n",
        "  dz = da * (1 - np.square(a))\n",
        "\n",
        "  grads['dba'] = np.sum(dz, axis=-1, keepdims=True)\n",
        "  grads['dWaa'] = dz @ a_prev.T # it is basically a sum over mini-batch sample gradients\n",
        "  grads['da_prev'] = Waa.T @ dz # shape (n_a, m)\n",
        "\n",
        "  grads['dWax'] = dz @ x.T\n",
        "  grads['dx'] = Wax.T @ dz\n",
        "\n",
        "  return grads"
      ],
      "metadata": {
        "id": "I7nLz-upEfaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_backward(da, caches):\n",
        "  '''\n",
        "  Implements the backward pass for the entire RNN\n",
        "\n",
        "  Args:\n",
        "  da -- dJ/da (from softmax) of shape (n_a, m, T_x) computed elsewhere\n",
        "  caches -- output from rnn()\n",
        "\n",
        "  Returns:\n",
        "  grads -- python dictionary with keys \n",
        "    'dx' : shape(n_x, m, T_x)\n",
        "    'da_init' : shape(n_a, m)\n",
        "    'dWax' : shape(n_a, n_x)\n",
        "    'dWaa' : shape(n_a, n_a)\n",
        "    'dba' : shape(n_a, 1)\n",
        "  '''\n",
        "  caches, x = caches\n",
        "  a, a_init, x, parameters = caches[0]\n",
        "\n",
        "  n_a, m, T_x = da.shape\n",
        "  n_x, _ = x.shape\n",
        "\n",
        "  dx = np.empty((n_x, m, 0))\n",
        "  dWax = np.zeros((n_a, n_x))\n",
        "  dWaa = np.zeros((n_a, n_a))\n",
        "  dba = np.zeros((n_a, 1))\n",
        "\n",
        "  # to save the last value\n",
        "  da_prev = np.zeros((n_a, m))\n",
        "\n",
        "  for time_step in range(T_x - 1, -1, -1):\n",
        "    grads = rnn_unit_backward(da[:, :, time_step] + da_prev, caches[time_step])\n",
        "    dba_t, dWaa_t, da_prev, dWax_t, dx_t = grads['dba'], grads['dWaa'], \\\n",
        "        grads['da_prev'], grads['dWax'], grads['dx']\n",
        "    \n",
        "    dx = np.append(dx_t[..., np.newaxis], dx, axis=-1)\n",
        "    dWax += dWax_t\n",
        "    dWaa += dWaa_t\n",
        "    dba += dba_t\n",
        "  \n",
        "  grads = {\"dx\": dx, \"da0\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n",
        "\n",
        "  return grads"
      ],
      "metadata": {
        "id": "eeD0JfrHNRxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM (Long Short-Term Memory) Network"
      ],
      "metadata": {
        "id": "ZTXDyTdyjRK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM unit"
      ],
      "metadata": {
        "id": "NlOt6FwuwQSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\tilde c^{<t>} = tanh(W_c[a^{<t-1>}, x^{<t>}] + b_c)$\n",
        "\n",
        "$\\Gamma_u=\\sigma(W_u[a^{<t-1>}, x^{<t>}] + b_u)\\text{   } (update)$\n",
        "\n",
        "$\\Gamma_f=\\sigma(W_f[a^{<t-1>}, x^{<t>}] + b_f) \\text{   } (forget)$\n",
        "\n",
        "$\\Gamma_o=\\sigma(W_o[a^{<t-1>}, x^{<t>}] + b_o) \\text{   } (output)$\n",
        "\n",
        "$c^{<t>} = \\Gamma_u*\\tilde c^{<t>} + \\Gamma f *c^{<t-1>}$\n",
        "\n",
        "$a^{<t>} = \\Gamma _o *tanh(c^{<t>})$\n",
        "\n",
        "\n",
        "![image](https://i.imgur.com/bT5bsFa.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "uuiM5GvejwYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_unit(x, a_prev, c_prev, parameters):\n",
        "  '''\n",
        "  Implements a single lstm time step\n",
        "\n",
        "  Args:\n",
        "  x -- input data of shape (n_x, m)\n",
        "  a_prev -- previous hidden state of shape (n_a, m)\n",
        "  c_prev -- previous memory state of shape (n_a, m)\n",
        "  parameters -- python dictionary with keys: 'Wc', 'bc', 'Wu', 'bu', 'Wf', 'bf',\n",
        "       'Wo', 'bo', 'Wy', 'by'\n",
        "\n",
        "  Returns:\n",
        "  y_pred -- predictions of shape (n_y, m)\n",
        "  a -- current hidden state of shape (n_a, m)\n",
        "  c -- current memory state of shape (n_a, m)\n",
        "  cache -- (a, c, a_prev, c_prev, forget_gate, update_gate, candidate,\n",
        "      output_gate, x, parameters) - for backprop\n",
        "  '''\n",
        "  n_x, m = x.shape\n",
        "\n",
        "  Wc, bc, Wu, bu, Wf, bf, Wo, bo, Wy, by = parameters['Wc'], parameters['bc'], \\\n",
        "      parameters['Wu'], parameters['bu'], parameters['Wf'], parameters['bf'], \\\n",
        "      parameters['Wo'], parameters['bo'], parameters['Wy'], parameters['by']\n",
        "\n",
        "  n_y, n_a = Wy.shape\n",
        "  concatenated = np.concatenate((a_prev, x), axis=0)\n",
        "\n",
        "  candidate = np.tanh(Wc @ concatenated + bc)\n",
        "\n",
        "  update_gate = sigmoid(Wu @ concatenated + bu)\n",
        "  forget_gate = sigmoid(Wf @ concatenated + bf)\n",
        "  output_gate = sigmoid(Wo @ concatenated + bo)\n",
        "\n",
        "  c = update_gate * candidate + forget_gate * c_prev\n",
        "  a = output_gate * np.tanh(c)\n",
        "\n",
        "  y_pred = scipy.special.softmax(Wy @ a + by, axis=0)\n",
        "  cache = (a, c, a_prev, c_prev, forget_gate, update_gate, candidate, \\\n",
        "      output_gate, x, parameters)\n",
        "  \n",
        "  return y_pred, a, c, cache"
      ],
      "metadata": {
        "id": "Q_PC8x5Fx9cn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM forward pass\n",
        "\n",
        "![image](https://i.imgur.com/SJQeZgS.png)"
      ],
      "metadata": {
        "id": "swYOjQ6dwSky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm(X, a_init, parameters):\n",
        "  '''\n",
        "  Implements lstm forward propagation\n",
        "\n",
        "  Args:\n",
        "  X -- input data of shape (n_x, m, T_x)\n",
        "  a_init -- initial hidden state\n",
        "  parameters -- python dictionary, same as above\n",
        "\n",
        "  Returns:\n",
        "  Y_pred -- predictions of shape (n_y, m, T_x)\n",
        "  A -- hidden states of shape (n_a, m, T_x)\n",
        "  C -- memory cells of shape (n_a, m, T_x)\n",
        "  caches -- ([caches], X)\n",
        "  '''\n",
        "  n_x, m, T_x = X.shape\n",
        "  n_y, n_a = parameters['Wy'].shape\n",
        "\n",
        "  Y_pred = np.empty((n_y, m, 0))\n",
        "  A = np.empty((n_a, m, 0))\n",
        "  C = np.empty((n_a, m, 0))\n",
        "  caches = []\n",
        "\n",
        "  a = a_init\n",
        "  c = np.zeros((n_a, m))\n",
        "\n",
        "  for time_step in range(T_x):\n",
        "\n",
        "    y_pred, a, c, cache = lstm_unit(X[:, :, time_step], a, c, parameters)\n",
        "    Y_pred = np.concatenate((Y_pred, y_pred[..., np.newaxis]), axis=-1)\n",
        "    A = np.concatenate((A, a[..., np.newaxis]), axis=-1)\n",
        "    C = np.concatenate((C, c[..., np.newaxis]), axis=-1)\n",
        "    caches.append(cache)\n",
        "\n",
        "  return Y_pred, A, C, (caches, X)"
      ],
      "metadata": {
        "id": "NDDlAYDpvYuu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM backpropagation\n",
        "\n",
        "![image](https://i.imgur.com/GWuqEdC.png)\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "d\\gamma_o^{\\langle t \\rangle} &= da_{next}*\\tanh(c_{next}) * \\Gamma_o^{\\langle t \\rangle}*\\left(1-\\Gamma_o^{\\langle t \\rangle}\\right)\\tag{7} \\\\[8pt]\n",
        "dp\\widetilde{c}^{\\langle t \\rangle} &= \\left(dc_{next}*\\Gamma_u^{\\langle t \\rangle}+ \\Gamma_o^{\\langle t \\rangle}* (1-\\tanh^2(c_{next})) * \\Gamma_u^{\\langle t \\rangle} * da_{next} \\right) * \\left(1-\\left(\\widetilde c^{\\langle t \\rangle}\\right)^2\\right) \\tag{8} \\\\[8pt]\n",
        "d\\gamma_u^{\\langle t \\rangle} &= \\left(dc_{next}*\\widetilde{c}^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle}* (1-\\tanh^2(c_{next})) * \\widetilde{c}^{\\langle t \\rangle} * da_{next}\\right)*\\Gamma_u^{\\langle t \\rangle}*\\left(1-\\Gamma_u^{\\langle t \\rangle}\\right)\\tag{9} \\\\[8pt]\n",
        "d\\gamma_f^{\\langle t \\rangle} &= \\left(dc_{next}* c_{prev} + \\Gamma_o^{\\langle t \\rangle} * (1-\\tanh^2(c_{next})) * c_{prev} * da_{next}\\right)*\\Gamma_f^{\\langle t \\rangle}*\\left(1-\\Gamma_f^{\\langle t \\rangle}\\right)\\tag{10}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "$dW_f = d\\gamma_f^{\\langle t \\rangle} \\begin{bmatrix} a_{prev} \\\\ x_t\\end{bmatrix}^T \\tag{11}$\n",
        "\n",
        "$dW_u = d\\gamma_u^{\\langle t \\rangle} \\begin{bmatrix} a_{prev} \\\\ x_t\\end{bmatrix}^T \\tag{12}$\n",
        "\n",
        "$dW_c = dp\\widetilde c^{\\langle t \\rangle} \\begin{bmatrix} a_{prev} \\\\ x_t\\end{bmatrix}^T \\tag{13}$\n",
        "\n",
        "$dW_o = d\\gamma_o^{\\langle t \\rangle} \\begin{bmatrix} a_{prev} \\\\ x_t\\end{bmatrix}^T \\tag{14}$\n",
        "\n",
        "$\\displaystyle db_f = \\sum_{batch}d\\gamma_f^{\\langle t \\rangle}\\tag{15}$\n",
        "\n",
        "$\\displaystyle db_u = \\sum_{batch}d\\gamma_u^{\\langle t \\rangle}\\tag{16}$\n",
        "\n",
        "$\\displaystyle db_c = \\sum_{batch}dp\\tilde c^{\\langle t \\rangle}\\tag{17}$\n",
        "\n",
        "$\\displaystyle db_o = \\sum_{batch}d\\gamma_o^{\\langle t \\rangle}\\tag{18}$\n",
        "\n",
        "$da_{prev} = W_f^T d\\gamma_f^{\\langle t \\rangle} + W_u^T   d\\gamma_u^{\\langle t \\rangle}+ W_c^T dp\\widetilde c^{\\langle t \\rangle} + W_o^T d\\gamma_o^{\\langle t \\rangle} \\tag{19}$\n",
        "\n",
        "$dc_{prev} = dc_{next}*\\Gamma_f^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle} * (1- \\tanh^2(c_{next}))*\\Gamma_f^{\\langle t \\rangle}*da_{next} \\tag{20}$\n",
        "\n",
        "$dx^{\\langle t \\rangle} = W_f^T d\\gamma_f^{\\langle t \\rangle} + W_u^T  d\\gamma_u^{\\langle t \\rangle}+ W_c^T dp\\widetilde c^{\\langle t \\rangle} + W_o^T d\\gamma_o^{\\langle t \\rangle}\\tag{21}$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CkdYOsEK47pF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_unit_backward(da, dc, cache):\n",
        "  '''\n",
        "  Implements backward pass for a signle LSTM unit\n",
        "\n",
        "  Args:\n",
        "  da -- dJ/da_next\n",
        "  dc -- dJ/dc_next\n",
        "  cache -- output from LSTM unit forward prop\n",
        "\n",
        "  Returns:\n",
        "  grads with keys: 'dx', 'da_prev', 'dc_prev', 'dWf', 'dbf', 'dWu', 'dbu', \n",
        "      'dWc', 'dbc', 'dWo', 'dbo'\n",
        "  '''\n",
        "  a, c, a_prev, c_prev, forget_gate, update_gate, candidate, \\\n",
        "      output_gate, x, parameters = cache\n",
        "\n",
        "  n_x, m = x.shape\n",
        "  n_a = parameters['Wc'].shape[0]\n",
        "\n",
        "  d_output_gate = da * np.tanh(c) * output_gate * (1 - output_gate)\n",
        "  d_candidate = (dc * update_gate + output_gate * (1 - np.tanh(c) ** 2) * \\\n",
        "      update_gate * da) * (1 - candidate ** 2)\n",
        "  d_update_gate = (dc * candidate + output_gate * (1 - np.tanh(c) ** 2) * \\\n",
        "      candidate * da) * update_gate * (1 - update_gate)\n",
        "  d_forget_gate = (dc * c_prev + output_gate * (1 - np.tanh(c) ** 2) * \\\n",
        "      c_prev * da) * forget_gate * (1 - forget_gate)\n",
        "\n",
        "  concatenated = np.concatenate((a_prev, x), axis=0)\n",
        "\n",
        "  dWf = d_forget_gate @ concatenated.T\n",
        "  dWu = d_update_gate @ concatenated.T\n",
        "  dWc = d_candidate @ concatenated.T\n",
        "  dWo = d_output_gate @ concatenated.T\n",
        "\n",
        "  dbf = np.sum(d_forget_gate, axis=1, keepdims=True)\n",
        "  dbu = np.sum(d_update_gate, axis=1, keepdims=True)\n",
        "  dbc = np.sum(d_candidate, axis=1, keepdims=True)\n",
        "  dbo = np.sum(d_output_gate, axis=1, keepdims=True)\n",
        "\n",
        "  da_prev = parameters['Wf'][:, :n_a].T @ d_forget_gate + parameters['Wu'][:, :n_a].T @ d_update_gate + \\\n",
        "      parameters['Wc'][:, :n_a].T @ d_candidate + parameters['Wo'][:, :n_a].T @ d_output_gate\n",
        "\n",
        "  dc_prev = dc * forget_gate + output_gate * (1 - np.tanh(c) ** 2) * \\\n",
        "      forget_gate * da\n",
        "  dx = parameters['Wf'][:, n_a :].T @ d_forget_gate + parameters['Wu'][:, n_a :].T @ d_update_gate + \\\n",
        "      parameters['Wc'][:, n_a :].T @ d_candidate + parameters['Wo'][:, n_a :].T @ d_output_gate\n",
        "\n",
        "  grads = {\"dxt\": dx, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf,\"dbf\": dbf, \"dWu\": dWu,\"dbu\": dbu,\n",
        "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
        "\n",
        "  return grads"
      ],
      "metadata": {
        "id": "Ua6iCCW0y1sn"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_backward(da, caches):\n",
        "  '''\n",
        "  Implements LSTM backward pass\n",
        "\n",
        "  Args:\n",
        "  da -- dJ/da from Dense -> softmax\n",
        "  caches -- output from lstm()\n",
        "\n",
        "  Returns:\n",
        "  grads with keys: 'dx', 'da_prev', 'dc_prev', 'dWf', 'dbf', 'dWu', 'dbu', \n",
        "      'dWc', 'dbc', 'dWo', 'dbo'\n",
        "  '''\n",
        "  caches, X = caches\n",
        "  a, c, a_prev, c_prev, forget_gate, update_gate, candidate, \\\n",
        "      output_gate, x, parameters = caches[0]\n",
        "\n",
        "  n_x, m, T_x = X.shape\n",
        "  n_a, m, T_x = da.shape\n",
        "  #n_a = parameters['Wc'].shape[0]\n",
        "\n",
        "  dx = np.empty((n_x, m, 0))\n",
        "  a_init = np.zeros((n_a, m))\n",
        "\n",
        "  dWf = np.zeros((n_a, n_a + n_x))\n",
        "  dWu = np.zeros((n_a, n_a + n_x))\n",
        "  dWc = np.zeros((n_a, n_a + n_x))\n",
        "  dWo = np.zeros((n_a, n_a + n_x))\n",
        "\n",
        "  dbf = np.zeros((n_a, 1))\n",
        "  dbu = np.zeros((n_a, 1))\n",
        "  dbc = np.zeros((n_a, 1))\n",
        "  dbo = np.zeros((n_a, 1))\n",
        "\n",
        "  da_next = np.zeros_like(da[:, :, 0])\n",
        "  dc = np.zeros_like(da[:, :, 0])\n",
        "\n",
        "  for time_step in range(T_x - 1, -1, -1):\n",
        "    grads = lstm_unit_backward(da[:, :, time_step] + da_next, dc, caches[time_step])\n",
        "    \n",
        "    da_next = grads['da_prev']\n",
        "    dc = grads['dc_prev']\n",
        "    dx = np.concatenate((grads['dxt'][..., np.newaxis], dx), axis=-1)\n",
        "\n",
        "    dWf += grads['dWf']\n",
        "    dWu += grads['dWu']\n",
        "    dWc += grads['dWc']\n",
        "    dWo += grads['dWo']\n",
        "\n",
        "    dbf += grads['dbf']\n",
        "    dbu += grads['dbu']\n",
        "    dbc += grads['dbc']\n",
        "    dbo += grads['dbo']\n",
        "\n",
        "  grads = {\"dx\": dx, \"da_init\": da_next, \"dWf\": dWf,\"dbf\": dbf, \"dWu\": dWu,\n",
        "               \"dbu\": dbu, \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
        "\n",
        "  return grads "
      ],
      "metadata": {
        "id": "uA--103eEacD"
      },
      "execution_count": 60,
      "outputs": []
    }
  ]
}