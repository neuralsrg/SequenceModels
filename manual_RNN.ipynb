{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "manual_RNN.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOuZi2Ph9m9frjSzwSo/FcI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuralsrg/SequenceModels/blob/main/manual_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def initialize_adam(parameters) :\n",
        "    \"\"\"\n",
        "    Initializes v and s as two python dictionaries with:\n",
        "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
        "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters.\n",
        "                    parameters[\"W\" + str(l)] = Wl\n",
        "                    parameters[\"b\" + str(l)] = bl\n",
        "    \n",
        "    Returns: \n",
        "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
        "                    v[\"dW\" + str(l)] = ...\n",
        "                    v[\"db\" + str(l)] = ...\n",
        "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
        "                    s[\"dW\" + str(l)] = ...\n",
        "                    s[\"db\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    s = {}\n",
        "    \n",
        "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
        "    for l in range(L):\n",
        "    ### START CODE HERE ### (approx. 4 lines)\n",
        "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
        "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
        "        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
        "        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return v, s\n",
        "\n",
        "\n",
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
        "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
        "    \"\"\"\n",
        "    Update parameters using Adam\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- python dictionary containing your gradients for each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    learning_rate -- the learning rate, scalar.\n",
        "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
        "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
        "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
        "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
        "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
        "    \n",
        "    # Perform Adam update on all parameters\n",
        "    for l in range(L):\n",
        "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)] \n",
        "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)] \n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1**t)\n",
        "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1**t)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * (grads[\"dW\" + str(l+1)] ** 2)\n",
        "        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * (grads[\"db\" + str(l+1)] ** 2)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2 ** t)\n",
        "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2 ** t)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / np.sqrt(s_corrected[\"dW\" + str(l+1)] + epsilon)\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / np.sqrt(s_corrected[\"db\" + str(l+1)] + epsilon)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    return parameters, v, s"
      ],
      "metadata": {
        "id": "6b7_6uYGvKt0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def rnn_cell_forward_tests(target):\n",
        "    # Only bias in expression\n",
        "    a_prev_tmp = np.zeros((5, 10))\n",
        "    xt_tmp = np.zeros((3, 10))\n",
        "    parameters_tmp = {}\n",
        "    parameters_tmp['Waa'] = np.random.randn(5, 5)\n",
        "    parameters_tmp['Wax'] = np.random.randn(5, 3)\n",
        "    parameters_tmp['Wya'] = np.random.randn(2, 5)\n",
        "    parameters_tmp['ba'] = np.random.randn(5, 1)\n",
        "    parameters_tmp['by'] = np.random.randn(2, 1)\n",
        "    parameters_tmp['Wya'] = np.zeros((2, 5))\n",
        "\n",
        "    a_next_tmp, yt_pred_tmp, cache_tmp = target(xt_tmp, a_prev_tmp, parameters_tmp)\n",
        "    \n",
        "    assert a_next_tmp.shape == (5, 10), f\"Wrong shape for a_next. Expected (5, 10) != {a_next_tmp.shape}\"\n",
        "    assert yt_pred_tmp.shape == (2, 10), f\"Wrong shape for yt_pred. Expected (2, 10) != {yt_pred_tmp.shape}\"\n",
        "    assert cache_tmp[0].shape == (5, 10), \"Wrong shape in cache->a_next\"\n",
        "    assert cache_tmp[1].shape == (5, 10), \"Wrong shape in cache->a_prev\"\n",
        "    assert cache_tmp[2].shape == (3, 10), \"Wrong shape in cache->x_t\"\n",
        "    assert len(cache_tmp[3].keys()) == 5, \"Wrong number of parameters in cache. Expected 5\"\n",
        "    \n",
        "    assert np.allclose(np.tanh(parameters_tmp['ba']), a_next_tmp), \"Problem 1 in a_next expression. Related to ba?\"\n",
        "    assert np.allclose(softmax(parameters_tmp['by']), yt_pred_tmp), \"Problem 1 in yt_pred expression. Related to by?\"\n",
        "\n",
        "    # Only xt in expression\n",
        "    a_prev_tmp = np.zeros((5,10))\n",
        "    xt_tmp = np.random.randn(3,10)\n",
        "    parameters_tmp['Wax'] = np.random.randn(5,3)\n",
        "    parameters_tmp['ba'] = np.zeros((5,1))\n",
        "    parameters_tmp['by'] = np.zeros((2,1))\n",
        "\n",
        "    a_next_tmp, yt_pred_tmp, cache_tmp = target(xt_tmp, a_prev_tmp, parameters_tmp)\n",
        "\n",
        "    assert np.allclose(np.tanh(np.dot(parameters_tmp['Wax'], xt_tmp)), a_next_tmp), \"Problem 2 in a_next expression. Related to xt?\"\n",
        "    assert np.allclose(softmax(np.dot(parameters_tmp['Wya'], a_next_tmp)), yt_pred_tmp), \"Problem 2 in yt_pred expression. Related to a_next?\"\n",
        "\n",
        "    # Only a_prev in expression\n",
        "    a_prev_tmp = np.random.randn(5,10)\n",
        "    xt_tmp = np.zeros((3,10))\n",
        "    parameters_tmp['Waa'] = np.random.randn(5,5)\n",
        "    parameters_tmp['ba'] = np.zeros((5,1))\n",
        "    parameters_tmp['by'] = np.zeros((2,1))\n",
        "\n",
        "    a_next_tmp, yt_pred_tmp, cache_tmp = target(xt_tmp, a_prev_tmp, parameters_tmp)\n",
        "\n",
        "    assert np.allclose(np.tanh(np.dot(parameters_tmp['Waa'], a_prev_tmp)), a_next_tmp), \"Problem 3 in a_next expression. Related to a_prev?\"\n",
        "    assert np.allclose(softmax(np.dot(parameters_tmp['Wya'], a_next_tmp)), yt_pred_tmp), \"Problem 3 in yt_pred expression. Related to a_next?\"\n",
        "\n",
        "    print(\"\\033[92mAll tests passed\")\n",
        "    \n",
        "\n",
        "def rnn_forward_test(target):\n",
        "    np.random.seed(17)\n",
        "    T_x = 13\n",
        "    m = 8\n",
        "    n_x = 4\n",
        "    n_a = 7\n",
        "    n_y = 3\n",
        "    x_tmp = np.random.randn(n_x, m, T_x)\n",
        "    a0_tmp = np.random.randn(n_a, m)\n",
        "    parameters_tmp = {}\n",
        "    parameters_tmp['Waa'] = np.random.randn(n_a, n_a)\n",
        "    parameters_tmp['Wax'] = np.random.randn(n_a, n_x)\n",
        "    parameters_tmp['Wya'] = np.random.randn(n_y, n_a)\n",
        "    parameters_tmp['ba'] = np.random.randn(n_a, 1)\n",
        "    parameters_tmp['by'] = np.random.randn(n_y, 1)\n",
        "\n",
        "    a, y_pred, caches = target(x_tmp, a0_tmp, parameters_tmp)\n",
        "    \n",
        "    assert a.shape == (n_a, m, T_x), f\"Wrong shape for a. Expected: ({n_a, m, T_x}) != {a.shape}\"\n",
        "    assert y_pred.shape == (n_y, m, T_x), f\"Wrong shape for y_pred. Expected: ({n_y, m, T_x}) != {y_pred.shape}\"\n",
        "    assert len(caches[0]) == T_x, f\"len(cache) must be T_x = {T_x}\"\n",
        "    \n",
        "    assert np.allclose(a[5, 2, 2:6], [0.99999291, 0.99332189, 0.9921928, 0.99503445]), \"Wrong values for a\"\n",
        "    assert np.allclose(y_pred[2, 1, 1: 5], [0.19428, 0.14292, 0.24993, 0.00119], atol=1e-4), \"Wrong values for y_pred\"\n",
        "    assert np.allclose(caches[1], x_tmp), f\"Fail check: cache[1] != x_tmp\"\n",
        "\n",
        "    \n",
        "    print(\"\\033[92mAll tests passed\")\n",
        "    \n",
        "def lstm_cell_forward_test(target):\n",
        "    np.random.seed(212)\n",
        "    m = 8\n",
        "    n_x = 4\n",
        "    n_a = 7\n",
        "    n_y = 3\n",
        "    x = np.random.randn(n_x, m)\n",
        "    a0 = np.random.randn(n_a, m)\n",
        "    c0 = np.random.randn(n_a, m)\n",
        "    params = {}\n",
        "    params['Wf'] = np.random.randn(n_a, n_a + n_x)\n",
        "    params['bf'] = np.random.randn(n_a, 1)\n",
        "    params['Wi'] = np.random.randn(n_a, n_a + n_x)\n",
        "    params['bi'] = np.random.randn(n_a, 1)\n",
        "    params['Wo'] = np.random.randn(n_a, n_a + n_x)\n",
        "    params['bo'] = np.random.randn(n_a, 1)\n",
        "    params['Wc'] = np.random.randn(n_a, n_a + n_x)\n",
        "    params['bc'] = np.random.randn(n_a, 1)\n",
        "    params['Wy'] = np.random.randn(n_y, n_a)\n",
        "    params['by'] = np.random.randn(n_y, 1)\n",
        "    a_next, c_next, y_pred, cache = target(x, a0, c0, params)\n",
        "    \n",
        "    assert len(cache) == 10, \"Don't change the cache\"\n",
        "    \n",
        "    assert cache[4].shape == (n_a, m), f\"Wrong shape for cache[4](ft). {cache[4].shape} != {(n_a, m)}\"\n",
        "    assert cache[5].shape == (n_a, m), f\"Wrong shape for cache[5](it). {cache[5].shape} != {(n_a, m)}\"\n",
        "    assert cache[6].shape == (n_a, m), f\"Wrong shape for cache[6](cct). {cache[6].shape} != {(n_a, m)}\"\n",
        "    assert cache[1].shape == (n_a, m), f\"Wrong shape for cache[1](c_next). {cache[1].shape} != {(n_a, m)}\"\n",
        "    assert cache[7].shape == (n_a, m), f\"Wrong shape for cache[7](ot). {cache[7].shape} != {(n_a, m)}\"\n",
        "    assert cache[0].shape == (n_a, m), f\"Wrong shape for cache[0](a_next). {cache[0].shape} != {(n_a, m)}\"\n",
        "    assert cache[8].shape == (n_x, m), f\"Wrong shape for cache[8](xt). {cache[8].shape} != {(n_x, m)}\"\n",
        "    assert cache[2].shape == (n_a, m), f\"Wrong shape for cache[2](a_prev). {cache[2].shape} != {(n_a, m)}\"\n",
        "    assert cache[3].shape == (n_a, m), f\"Wrong shape for cache[3](c_prev). {cache[3].shape} != {(n_a, m)}\"\n",
        "    \n",
        "    assert a_next.shape == (n_a, m), f\"Wrong shape for a_next. {a_next.shape} != {(n_a, m)}\"\n",
        "    assert c_next.shape == (n_a, m), f\"Wrong shape for c_next. {c_next.shape} != {(n_a, m)}\"\n",
        "    assert y_pred.shape == (n_y, m), f\"Wrong shape for y_pred. {y_pred.shape} != {(n_y, m)}\"\n",
        "\n",
        "    \n",
        "    assert np.allclose(cache[4][0, 0:2], [0.32969833, 0.0574555]), \"wrong values for ft\"\n",
        "    assert np.allclose(cache[5][0, 0:2], [0.0036446, 0.9806943]), \"wrong values for it\"\n",
        "    assert np.allclose(cache[6][0, 0:2], [0.99903873, 0.57509956]), \"wrong values for cct\"\n",
        "    assert np.allclose(cache[1][0, 0:2], [0.1352798,  0.39884899]), \"wrong values for c_next\"\n",
        "    assert np.allclose(cache[7][0, 0:2], [0.7477249,  0.71588751]), \"wrong values for ot\"\n",
        "    assert np.allclose(cache[0][0, 0:2], [0.10053951, 0.27129536]), \"wrong values for a_next\"\n",
        "    \n",
        "    assert np.allclose(y_pred[1], [0.417098, 0.449528, 0.223159, 0.278376,\n",
        "                                   0.68453,  0.419221, 0.564025, 0.538475]), \"Wrong values for y_pred\"\n",
        "    \n",
        "    print(\"\\033[92mAll tests passed\")\n",
        "    \n",
        "def lstm_forward_test(target):\n",
        "    np.random.seed(45)\n",
        "    n_x = 4\n",
        "    m = 13\n",
        "    T_x = 16\n",
        "    n_a = 3\n",
        "    n_y = 2\n",
        "    x_tmp = np.random.randn(n_x, m, T_x)\n",
        "    a0_tmp = np.random.randn(n_a, m)\n",
        "    parameters_tmp = {}\n",
        "    parameters_tmp['Wf'] = np.random.randn(n_a, n_a + n_x)\n",
        "    parameters_tmp['bf'] = np.random.randn(n_a, 1)\n",
        "    parameters_tmp['Wi'] = np.random.randn(n_a, n_a + n_x)\n",
        "    parameters_tmp['bi']= np.random.randn(n_a, 1)\n",
        "    parameters_tmp['Wo'] = np.random.randn(n_a, n_a + n_x)\n",
        "    parameters_tmp['bo'] = np.random.randn(n_a, 1)\n",
        "    parameters_tmp['Wc'] = np.random.randn(n_a, n_a + n_x)\n",
        "    parameters_tmp['bc'] = np.random.randn(n_a, 1)\n",
        "    parameters_tmp['Wy'] = np.random.randn(n_y, n_a)\n",
        "    parameters_tmp['by'] = np.random.randn(n_y, 1)\n",
        "\n",
        "    a, y, c, caches = target(x_tmp, a0_tmp, parameters_tmp)\n",
        "    \n",
        "    assert a.shape == (n_a, m, T_x), f\"Wrong shape for a. {a.shape} != {(n_a, m, T_x)}\"\n",
        "    assert c.shape == (n_a, m, T_x), f\"Wrong shape for c. {c.shape} != {(n_a, m, T_x)}\"\n",
        "    assert y.shape == (n_y, m, T_x), f\"Wrong shape for y. {y.shape} != {(n_y, m, T_x)}\"\n",
        "    assert len(caches[0]) == T_x, f\"Wrong shape for caches. {len(caches[0])} != {T_x} \"\n",
        "    assert len(caches[0][0]) == 10, f\"length of caches[0][0] must be 10.\"\n",
        "    \n",
        "    assert np.allclose(a[2, 1, 4:6], [-0.01606022,  0.0243569]), \"Wrong values for a\"\n",
        "    assert np.allclose(c[2, 1, 4:6], [-0.02753855,  0.05668358]), \"Wrong values for c\"\n",
        "    assert np.allclose(y[1, 1, 4:6], [0.70444592 ,0.70648935]), \"Wrong values for y\"\n",
        "    \n",
        "    print(\"\\033[92mAll tests passed\")"
      ],
      "metadata": {
        "id": "gPQ3ePOevFcu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual implementation of RNN in numpy"
      ],
      "metadata": {
        "id": "NWNdRPg_rnkr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "U2P7n0bJrkt1"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN"
      ],
      "metadata": {
        "id": "PBMqDwlMGzPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN unit\n",
        "![image](https://i.imgur.com/7PAU2pP.png)"
      ],
      "metadata": {
        "id": "zqBTQ5k5r9lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_unit(x, a_prev, parameters):\n",
        "  '''\n",
        "  Implements signle RNN unit forward pass\n",
        "\n",
        "  Args:\n",
        "  x -- input vector\n",
        "  a_prev -- activation from previous RNN unit\n",
        "  parameters -- python dictionary with keys 'Waa', 'Wax', 'ba', 'Wya', 'by'\n",
        "\n",
        "  Returns:\n",
        "  y_pred -- softmax activation outputs\n",
        "  a -- current RNN unit hidden state\n",
        "  cache -- tuple containing (a, a_prev, x, parameters)\n",
        "  '''\n",
        "  Waa, Wax, ba, Wya, by = parameters['Waa'], parameters['Wax'], parameters['ba'], \\\n",
        "                          parameters['Wya'], parameters['by']\n",
        "\n",
        "  a = Waa @ a_prev + Wax @ x + ba\n",
        "  a = np.tanh(a)\n",
        "\n",
        "  y_pred = scipy.special.softmax(Wya @ a + by, axis=0)\n",
        "\n",
        "  cache = (a, a_prev, x, parameters)\n",
        "\n",
        "  return y_pred, a, cache"
      ],
      "metadata": {
        "id": "l5qDDOfer0Hq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN forward pass"
      ],
      "metadata": {
        "id": "AzzwyTv1ze07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn(x, a_init, parameters):\n",
        "  '''\n",
        "  Implements RNN forward pass\n",
        "\n",
        "  Args:\n",
        "  x -- tensor of shape (n_x, m, T_x), where n_x - num of features, m - batch size,\n",
        "      T_x - num of time steps\n",
        "  a_init -- initial rnn hidden state\n",
        "  parameters -- python dictionary with keys 'Waa', 'Wax', 'ba', 'Wya', 'by'\n",
        "\n",
        "  Returns:\n",
        "  Y_pred -- tensor of shape (n_y, m, T_x)\n",
        "  A -- tensor of hidden states of shape (n_a, m, T_x)\n",
        "  caches -- (list_of_caches, x)\n",
        "  '''\n",
        "  n_x, m, T_x = x.shape\n",
        "  a = a_init\n",
        "  n_y, n_a = parameters['Wya'].shape\n",
        "\n",
        "  Y_pred = np.empty((n_y, m, 0))\n",
        "  A = np.empty((n_a, m, 0))\n",
        "  caches = []\n",
        "\n",
        "  for time_step in range(T_x):\n",
        "    y_pred, a, cache = rnn_unit(x[:, :, time_step], a, parameters)\n",
        "\n",
        "    Y_pred = np.append(Y_pred, y_pred[..., np.newaxis], axis=-1)\n",
        "    A = np.append(A, a[..., np.newaxis], axis=-1)\n",
        "    caches.append(cache)\n",
        "\n",
        "  return Y_pred, A, (caches, x)"
      ],
      "metadata": {
        "id": "bUZy2eVtx1tN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN backpropagation\n",
        "\n",
        "![image](https://i.imgur.com/x2jxcSq.png)"
      ],
      "metadata": {
        "id": "xREIaG1pG3uE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{align}\n",
        "\\displaystyle a^{\\langle t \\rangle} &= \\tanh(W_{ax} x^{\\langle t \\rangle} + W_{aa} a^{\\langle t-1 \\rangle} + b_{a})\\tag{-} \\\\[8pt]\n",
        "\\displaystyle \\frac{\\partial \\tanh(x)} {\\partial x} &= 1 - \\tanh^2(x) \\tag{-} \\\\[8pt]\n",
        "\\displaystyle {dtanh} &= da_{next} * ( 1 - \\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a})) \\tag{0} \\\\[8pt]\n",
        "\\displaystyle  {dW_{ax}} &= dtanh \\cdot x^{\\langle t \\rangle T}\\tag{1} \\\\[8pt]\n",
        "\\displaystyle dW_{aa} &= dtanh \\cdot a^{\\langle t-1 \\rangle T}\\tag{2} \\\\[8pt]\n",
        "\\displaystyle db_a& = \\sum_{batch}dtanh\\tag{3} \\\\[8pt]\n",
        "\\displaystyle dx^{\\langle t \\rangle} &= { W_{ax}}^T \\cdot dtanh\\tag{4} \\\\[8pt]\n",
        "\\displaystyle da_{prev} &= { W_{aa}}^T \\cdot dtanh\\tag{5}\n",
        "\\end{align}\n",
        "$$"
      ],
      "metadata": {
        "id": "pMRByF_vFUUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_unit_backward(da, cache):\n",
        "  '''\n",
        "  Implements the backward pass for a single rnn unit based on it's cache\n",
        "\n",
        "  Args:\n",
        "  da -- dJ/da (da_next from the picture above)\n",
        "  cache -- cache from rnn forward pass\n",
        "\n",
        "  Returns:\n",
        "  grads -- python dictionary with keys: 'dba', 'dWax', 'dx' (for deep RNNs),\n",
        "      'dWaa', 'dWa_prev', 'da_prev'\n",
        "  '''\n",
        "  a, a_prev, x, parameters = cache\n",
        "  Waa, Wax, ba, Wya, by = parameters['Waa'], parameters['Wax'], parameters['ba'], \\\n",
        "                          parameters['Wya'], parameters['by']\n",
        "\n",
        "  grads = {}\n",
        "\n",
        "  dz = da * (1 - np.square(a))\n",
        "\n",
        "  grads['dba'] = np.sum(dz, axis=-1, keepdims=True)\n",
        "  grads['dWaa'] = dz @ a_prev.T # it is basically a sum over mini-batch sample gradients\n",
        "  grads['da_prev'] = Waa.T @ dz # shape (n_a, m)\n",
        "\n",
        "  grads['dWax'] = dz @ x.T\n",
        "  grads['dx'] = Wax.T @ dz\n",
        "\n",
        "  return grads"
      ],
      "metadata": {
        "id": "I7nLz-upEfaP"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_backward(da, caches):\n",
        "  '''\n",
        "  Implements the backward pass for the entire RNN\n",
        "\n",
        "  Args:\n",
        "  da -- dJ/da of shape (n_a, m, T_x) computed elsewhere\n",
        "  caches -- output from rnn()\n",
        "\n",
        "  Returns:\n",
        "  grads -- python dictionary with keys \n",
        "    'dx' : shape(n_x, m, T_x)\n",
        "    'da_init' : shape(n_a, m)\n",
        "    'dWax' : shape(n_a, n_x)\n",
        "    'dWaa' : shape(n_a, n_a)\n",
        "    'dba' : shape(n_a, 1)\n",
        "  '''\n",
        "  caches, x = caches\n",
        "  a, a_init, x, parameters = caches[0]\n",
        "\n",
        "  n_a, m, T_x = da.shape\n",
        "  n_x, _ = x.shape\n",
        "\n",
        "  dx = np.empty((n_x, m, 0))\n",
        "  dWax = np.zeros((n_a, n_x))\n",
        "  dWaa = np.zeros((n_a, n_a))\n",
        "  dba = np.zeros((n_a, 1))\n",
        "\n",
        "  # to save the last value\n",
        "  da_prev = np.zeros((n_a, m))\n",
        "\n",
        "  for time_step in range(T_x - 1, -1, -1):\n",
        "    grads = rnn_unit_backward(da[:, :, time_step] + da_prev, caches[time_step])\n",
        "    dba_t, dWaa_t, da_prev, dWax_t, dx_t = grads['dba'], grads['dWaa'], \\\n",
        "        grads['da_prev'], grads['dWax'], grads['dx']\n",
        "    \n",
        "    dx = np.append(dx_t[..., np.newaxis], dx, axis=-1)\n",
        "    dWax += dWax_t\n",
        "    dWaa += dWaa_t\n",
        "    dba += dba_t\n",
        "  \n",
        "  grads = {\"dx\": dx, \"da0\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n",
        "\n",
        "  return grads"
      ],
      "metadata": {
        "id": "eeD0JfrHNRxT"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fbv1cQBoY7qS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}